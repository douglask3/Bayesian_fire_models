{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table style=\"width: 100%; border-collapse: collapse;\" border=\"0\">\n",
    "<tr>\n",
    "<td><b>Created:</b> Tuesday 31 January 2017</td>\n",
    "<td style=\"text-align: right;\"><a href=\"https://github.com/douglask3/amazon_fires\">github.com/douglask3/amazon_fires</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<div>\n",
    "<center>\n",
    "<font face=\"Times\">\n",
    "<br>\n",
    "<h1>Unusual fire seasons in a changing climate - A Bayesian approach</h1>\n",
    "<br>\n",
    "<br>\n",
    "<sup>1,* </sup> Douglas Ian Kelley,\n",
    "<sup>2 </sup>Chantelle Burton, \n",
    "<sup>3 </sup>Rhys Whitley,\n",
    "<sup>1 </sup>Chris Huntingford,\n",
    "<sup>4 </sup>Ioannis Bistinas, \n",
    "<sup>1,5 </sup>Megan Brown, \n",
    "<sup>6 </sup>Ning Dong, \n",
    "<sup>1 </sup>Toby R. Marthews\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<sup>*  </sup> douglas.i.kelley@gmail.com\n",
    "<br>\n",
    "<sup>1 </sup>Centre for Ecology and Hydrology, Maclean Building, Crowmarsh Gifford, Wallingford, Oxfordshire, United Kingdom\n",
    "<br>\n",
    "<sup>2 </sup>Met Office United Kingdom, Exeter, United Kingdom\n",
    "<br>\n",
    "<sup>3 </sup>Natural Perils Pricing, Commercial & Consumer Portfolio & Pricing, Suncorp Group, Sydney, Australia\n",
    "<br>\n",
    "<sup>4 </sup>ATOS Nederland B.V., Amstelveen, The Netherlands\n",
    "<br>\n",
    "<sup>5 </sup>School of Physical Sciences, The Open University, Milton Keynes, UK\n",
    "<br>\n",
    "<sup>6 </sup>Department of Biological Sciences, Macquarie University, North Ryde, NSW 2109, Australia \n",
    "<br>\n",
    "<br>\n",
    "<h3>Abstract</h3>\n",
    "<hr>\n",
    "<p> \n",
    "This notebook aims to quantify the model parameters of the ConFire model, defined below. The model is driven by a number of covariates (X<sub>i=1, 2, ... M</sub>) that describe: agricultural footprints; frequency of lightning ignitions, population density, vegetation cover, soil moisture at atmospheric drying potential. The model attempts to predict the impact of fire through burnt area and is thus the model target (Y).\n",
    "</p>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<i>Python code and calculations below</i>\n",
    "<br>\n",
    "</font>\n",
    "</center>\n",
    "<hr>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "Lets import the librarys and set plotting styles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from   io     import StringIO\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import math\n",
    "\n",
    "import pymc3  as pm3 \n",
    "from   pymc3.backends import SQLite\n",
    "from   scipy  import optimize\n",
    "from   theano import tensor as tt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import corner  # https://corner.readthedocs.io\n",
    "\n",
    "# setup nice plotting\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These paths are for input data (```datPath```), made using the previous prepare_data.ipynb notebook, and the resultant parameter distribution (```param_outpath```) which we'll need when we run ```make_model_output.ipynb``` next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "datDir        = \"../inputs/ISIMIP_inference/\"\n",
    "param_outpath = '../outputs/params-for_sampling'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model description\n",
    "Here is a brief model outline for the sake of comprehension of this notebook. The science behind each of these drivers can be found in the companion papers. Kelley et al. 2019 can be found here [https://rdcu.be/bO6ey](https://rdcu.be/bO6ey). The paper specific to Amazon fires should be available soon.\n",
    " \n",
    " The model considers fractional burnt area to be the joint product of a set of conditions that modulate fire through fuel load, ignitions, moisture and suppression. Each control assumes some equilibrium point that describes the optimal conditions for fire, that may be proportionally modified through some empirical relationship.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "    F_{burn} &=& F_{max} \\cdot \\prod_{i}S(x_{i}) \\\\[1em]\n",
    "\\end{eqnarray}\n",
    "\n",
    "Where $S(x_{i})$ representes some measure of fire conditions by $i =$ fuel, moisture, ignitions and anthropagenic supression, and is describe by a sigmoid:\n",
    "\n",
    "\\begin{equation}\n",
    "    S(x_{i=fuel, moist, ignite, suppr}) = \\frac{1}{1 + \\exp\\{-k_i\\cdot(x_i-x_{0,i})\\}}\n",
    "\\end{equation}\n",
    "\n",
    "The $fuel$ sigmoid considers fractional vegetation cover and seasonal fuel ``flushing''. Sigmoids $moist$, $ignite$ and $suppr$ describe an aggregation of other climate and land-use covariates. Because these sigmoids are influenced by an aggregation of different drivers, they are influenced in turn by different sets of hyper-parameters; these are now described below.  \n",
    "\n",
    "#### Fuel load covariate (no hyper-parameters)\n",
    "\\begin{equation}\n",
    "    x_{fuel} = FPC^{p} \\cdot (v_{fuel} \\cdot (\\alpha_{max}/\\alpha_{mean} -1) + 1)/(1+v_{fuel}) \n",
    "\\end{equation}\n",
    "\n",
    "#### Moisture covariate\n",
    "\\begin{equation}\n",
    "    x_{moist} = (\\alpha_{shallow} + v_{deep} \\cdot \\alpha_{deep} + v_M \\cdot EMC_{*} + v_{tree} \\cdot FPC_{tree}) / (1 + v_{deep} + v_M + v_{tree})\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    EMC_* = WD + (1-WD) * ECM\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    WD = 1 - e^{\\tau * precip}\n",
    "\\end{equation}\n",
    "\n",
    "where $WD$ is a proxy for wet days and $EMC$ is the equilibrium moisture content.\n",
    "#### Ignition covariate \n",
    "\\begin{equation}\n",
    "    x_{ignite} = (Lightn + v_p \\cdot A_{pasture} + v_{d1}\\cdot\\rho_{population}) / (1 + v_p + v_{d1})\n",
    "\\end{equation}\n",
    "\n",
    "Where $Lightn$ is the number of cloud-to-ground lightning strikes, modified as per Kelley et al. 2014.\n",
    "\n",
    "#### Supression covariate \n",
    "\\begin{equation}\n",
    "    x_{supress} = (A_{urban} + v_C\\cdot A_{Crop} + v_{d2}\\cdot\\rho_{population} ) / (1 + v_C + v_{d2})\n",
    "\\end{equation}\n",
    "\n",
    "This leaves 19 free parameters $\\{F_{max}, \\{k_i\\}, \\{x_{0, i}\\}, p, v_{fuel}, v_{deep}, v_M, v_{tree}, \\tau, v_p, v_{d1}, v_C, v_{d2}\\}$ that need to be optimised against observations of burnt area.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def npLogit(x):\n",
    "    return np.log(x/(1-x))\n",
    "    \n",
    "def ttLogit(x):\n",
    "    return tt.log(x/(1-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.1 Fire limitation model definition\n",
    "\n",
    "Could possibly contain this in a class object, but I'm not sure theano can instantiate the object to be used by the GPU. If I've made absolutely no sense just then, then I would leave the following as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdb import set_trace as browser\n",
    "def fuel_load(vegcover, soilwMax, fuel_pw, fuel_pg):\n",
    "    \"\"\"\n",
    "    Definition to describe fuel load: while return the input; capability to be modified later.\n",
    "    \"\"\"\n",
    "  \n",
    "    return (vegcover**fuel_pw) * (fuel_pg * (soilwMax-1) + 1) / (1 + fuel_pg)\n",
    "\n",
    "def emc_weighted(emc, precip, wd_pg):\n",
    "    wet_days = 1 - tt.exp(-wd_pg * precip)\n",
    "    \n",
    "    emcw = wet_days + (1-wet_days) * emc\n",
    "    \n",
    "    return(emcw)\n",
    "    \n",
    "    \n",
    "def moisture(shallow_soilw, deep_soilw, emcw, treeCover, cMs, cM, cMT, kM, pT):\n",
    "    \"\"\"\n",
    "    Definition to describe moisture\n",
    "    \"\"\"\n",
    "    moist = (shallow_soilw + cMs * deep_soilw + cM*emcw + cMT * (treeCover**pT)) / (1 + cM + cMs + cMT)\n",
    "    moist = 1 - tt.log(1 - moist*kM)\n",
    "    return moist\n",
    "\n",
    "                             \n",
    "def ignition(pasture):\n",
    "    \"\"\"\n",
    "    Definition for the measure of ignition\n",
    "    \"\"\"\n",
    "    ignite = pasture\n",
    "    #ignite = tt.log(ignite)\n",
    "    return ignite\n",
    "\n",
    "def supression(cropland):\n",
    "    \"\"\"\n",
    "    Definition for the measure of fire supression\n",
    "    \"\"\"\n",
    "    return cropland\n",
    "\n",
    "def tt_sigmoid(x, k, x0):\n",
    "    \"\"\"\n",
    "    Sigmoid function to describe limitation using tensor\n",
    "    \"\"\"\n",
    "    return 1.0/(1.0 + tt.exp(-k*(x - x0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Import data\n",
    "\n",
    "Load data and do any necessary transformation needed for the Bayesian modelling framework. Rows are defined as a fraction of total data points (above). For full optimization, we set at 100%, but for testing purposes, I've limited the number of rows to 10% and still got vaguely sensible test results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "                   # Full optiomization recomentations in comments. These can be reduced to test things out though\n",
    "sample_pc     = 1 # = 100 # Percentage of grid cells samples (largest = 100%) - training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_with_buffer(filename, line_select, **kwargs):\n",
    "    s_buf = StringIO()\n",
    "    line_select = np.sort(line_select)\n",
    "    with open(filename) as file:\n",
    "        count = -1\n",
    "        lineN = -1\n",
    "        for line in file:\n",
    "            lineN += 1\n",
    "            if lineN == 0 or lineN == line_select[count]:\n",
    "                s_buf.write(line)\n",
    "                \n",
    "                count += 1\n",
    "                if count == len(line_select): break\n",
    "            \n",
    "    s_buf.seek(0)\n",
    "    df = pd.read_csv(s_buf,**kwargs)\n",
    "    return df\n",
    "\n",
    "def file_len(fname):\n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f): pass\n",
    "    return i + 1\n",
    "\n",
    "\n",
    "def openDat(datPath):\n",
    "    datPath = datDir + '/' + datPath\n",
    "    DATAPATH = os.path.expanduser(datPath)\n",
    "\n",
    "    nlines      = file_len(DATAPATH)\n",
    "    npoints     = round(sample_pc * nlines / 100)\n",
    "    line_select = np.random.choice(range(0, nlines), npoints, False)\n",
    "    line_select = line_select[line_select > 0]\n",
    "    fd          = load_with_buffer(DATAPATH, line_select)\n",
    "    \n",
    "    BA = npLogit(fd[\"burnt_area\"].values)\n",
    "    BA[BA < -20] = -20\n",
    "    \n",
    "    fd[\"burnt_area\"].values[:] = BA[:]\n",
    "    \n",
    "    fd[\"soilM_bottom\"].values[:] = fd[\"soilM_bottom\"].values[:]/100\n",
    "    fd[\"soilM_top\"].values[:] = fd[\"soilM_top\"].values[:]/100\n",
    "    #obs = fd[\"burnt_area_MCD\"].values\n",
    "    #obs = npLogit(obs)\n",
    "    #obs[obs < -31]  = -31\n",
    "    return fd\n",
    "\n",
    "fds = [openDat(f) for f in os.listdir(datDir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>crop</th>\n",
       "      <th>humid</th>\n",
       "      <th>pas</th>\n",
       "      <th>precip</th>\n",
       "      <th>soil12</th>\n",
       "      <th>soilM_bottom</th>\n",
       "      <th>soilM_top</th>\n",
       "      <th>tas</th>\n",
       "      <th>totalVeg</th>\n",
       "      <th>trees</th>\n",
       "      <th>burnt_area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.500838</td>\n",
       "      <td>0.500838</td>\n",
       "      <td>243.985245</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>107</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000283</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.528277</td>\n",
       "      <td>0.528277</td>\n",
       "      <td>242.057648</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>174</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.076249</td>\n",
       "      <td>0.275096</td>\n",
       "      <td>0.275096</td>\n",
       "      <td>240.415207</td>\n",
       "      <td>0.275660</td>\n",
       "      <td>0.275654</td>\n",
       "      <td>-20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>390</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.083285</td>\n",
       "      <td>0.498921</td>\n",
       "      <td>0.498921</td>\n",
       "      <td>247.432205</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>391</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.075337</td>\n",
       "      <td>0.387578</td>\n",
       "      <td>0.387578</td>\n",
       "      <td>241.373703</td>\n",
       "      <td>0.004043</td>\n",
       "      <td>0.004036</td>\n",
       "      <td>-20.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      crop     humid       pas    precip    soil12  soilM_bottom  \\\n",
       "0          19  0.000002  0.000317  0.000002  0.000005  0.083333      0.500838   \n",
       "1         107  0.000002  0.000283  0.000002  0.000004  0.083333      0.528277   \n",
       "2         174  0.000002  0.000176  0.000002  0.000003  0.076249      0.275096   \n",
       "3         390  0.000002  0.000333  0.000298  0.000005  0.083285      0.498921   \n",
       "4         391  0.000002  0.000183  0.000003  0.000005  0.075337      0.387578   \n",
       "\n",
       "   soilM_top         tas  totalVeg     trees  burnt_area  \n",
       "0   0.500838  243.985245  0.000013  0.000007       -20.0  \n",
       "1   0.528277  242.057648  0.000013  0.000007       -20.0  \n",
       "2   0.275096  240.415207  0.275660  0.275654       -20.0  \n",
       "3   0.498921  247.432205  0.000309  0.000007       -20.0  \n",
       "4   0.387578  241.373703  0.004043  0.004036       -20.0  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fds[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Do a sanity check to make sure our data has imported correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Bayesian framework\n",
    "\n",
    "A simple explanation of Baye's law is:\n",
    "\n",
    "\\begin{equation}\n",
    "    P(\\beta|X) \\propto P(\\beta)\\cdot P(X|\\beta)\n",
    "\\end{equation}\n",
    "\n",
    "where $X$ is our data (observations of some arbitrary system), and $\\beta$ our set of unexplained parameters that describe the response of our _proposed understanding_ of this system as it varies with $X$.\n",
    "\n",
    "### 2.3.1 Prior definitions\n",
    "Because I have no idea what the uncertainty on the hyperparameters should look like (beyond, say $\\beta_i> 0$), I've made the priors as uninformed as possible, with distributions determined by physical bounds of the variable in question, or set generously beyond what is realistically plausible. Some of them could have more information or distributions based on updated qualitative knowledge of the bounds, but we can play around with that later.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "    P(\\beta) &=& \\prod_{i=1}^{4}P(a_i)\\prod_{i=1}^{4}P(b_i)\\cdot P(\\sigma)\\cdot P(v_c)P(v_p)P(v_{d,1})P(v_{d,2}) \\\\[1.5em]\n",
    "    P(a) = P(b) = P(\\sigma) &=& \\mathcal{N}(0, 1) \\\\[1em]\n",
    "    P(v_c) = P(v_p) = P(v_{d,1}) = P(v_{d,2}) &=& \\mathcal{U}(\\beta_{\\min}, \\beta_{\\max}) \\\\[1.5em]\n",
    "\\end{eqnarray}\n",
    "\n",
    "I'm not totally sure about the maths above being right, but it's just to show that a normal assumption for _full_ prior isn't crazy. Important, because we'll also describe the error (likelihood) as normal, such that the posterior is therefore normal (conjugate); i.e. $\\mathcal{N}\\times\\mathcal{N}=\\mathcal{N}$ (expansion happens in the mean of the exponent). In reality, the model could be better approximated with something a little cleverer than a normal distribution... but all our attempts at designing this so far have made convergence screwy. Again, more play required here.\n",
    "\n",
    "Back to the code.., `pymc3` is quite funky in that it allows me to create an empty `Model()` object and just add things to it as I need them using a `with` statement. I've called our Bayesian model `fire_error` as that is what we are trying to Quantify.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Likelihood definition\n",
    "\n",
    "For the sake of simplicity (and because I don't really know any better), we define the model error as normally distributed (i.i.d.) although it most likely isn't. We could make this more complicated later by defining the error as heteroscedastic, but I wouldn't bother with that until we have some idea of the convergence. We're describing the error (observations minus model predictions) as follows:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "    P(X|\\beta) &=& \\mathcal{N}(F_{burn}, \\sigma) \\\\[1em]\n",
    "    \\mathcal{N}(F_{burn}, \\sigma) &=& \\frac{N}{\\sigma\\sqrt{2\\pi}}\\exp\\left\\{\\sum_{i=1}^{N}\\left(\\frac{y_i - F_{burn, i}}{\\sigma_i}\\right)^2\\right\\}\n",
    "\\end{eqnarray}\n",
    "\n",
    "where $y_i$ is a set of observations we're attempting to optimise on. Below is the code that describes the above:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Posterior sampling\n",
    "\n",
    "Because it is nigh impossible to determine the posterior solution analytically we will instead sample the information space to **infer** the posterior solutions for each of the model parameters. In this case, we are using a Metropolis-Hasting step MCMC.\n",
    "\n",
    "I've tried using No-U-Turn (NUTS) sampling (which is the new kid on the block), but there are issues with it's current implementation in pymc3 (see github repo issues). Can use it once problems are ironed out (and as it's been a couple of years since this bit was written, many are probably fixed by now) - but TBH it doesn't matter if we're getting a reasonable convergence.\n",
    "\n",
    "The Metropolis-Hasting step MCMC is set by ```pm3.Metropolis()``` and sampling information is passed to ```pm3.sample```. The first argument is the number of iterations. 1000 should be okay to see if the samplers doing something sensible. 10,000 should be enough iteration to do quick rough tests of the model posterior. For full and safe (and publishable) results, best set to 100,000, though even for this, you should check trace convergence to make sure you have enough effective samples. If not, you might need up to 1,000,000+. The number of ```chains``` basically says how many times to run the sampler... important incase our MCMC technique gets \"stuck\" somewhere. For playing with, this doesn’t need to be particularly high, though I've set this at 10 to play it safe.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_zero_inflated_normal(mu, sigma, pz, x):\n",
    "    '''return tt.sw1itch(\n",
    "        tt.lt(x, -150),\n",
    "        -p0,\n",
    "        -(1.0 - p0) *(1.0/(sigma * 2.506))*tt.exp(-0.5 * ((x-mu)/sigma)**2)\n",
    "    )\n",
    "    '''\n",
    "    return tt.switch(\n",
    "        tt.lt(x, -30),\n",
    "        tt.log(pz),\n",
    "        tt.log(1-pz) - tt.log(sigma * tt.sqrt(2*math.pi)) - ((x-mu)**2)/(2*sigma**2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runInference(fd, outfile):\n",
    "\n",
    "    with pm3.Model() as fire_error:\n",
    "\n",
    "    # first for the sigmoids  \n",
    "        fuel_x0        = pm3.Normal     ('fuel_x0'     , 0.5, 0.25)\n",
    "        fuel_k         = pm3.Exponential('fuel_k'      , 1.0      ) \n",
    "        fuel_pw        = pm3.Exponential('fuel_pw'     , 1.0      )\n",
    "        fuel_pg        = pm3.Uniform('fuel_pg'         , 0.0, 1.0 )\n",
    "\n",
    "        moisture_x0    = pm3.Normal     ('moisture_x0', 0.5, 0.25)\n",
    "        moisture_k     = pm3.Exponential('moisture_k'  , 1.0     )\n",
    "        wd_pg          = pm3.Exponential('wd_pg'  , 1.0     )\n",
    "\n",
    "        ignition_x0    = pm3.Normal     ('ignition_x0', 1000.0, 50.0)\n",
    "        ignition_k     = pm3.Exponential('ignition_k' , 100.0     )\n",
    "\n",
    "        suppression_x0 = pm3.Normal ('suppression_x0'  , 0.5, 0.25)\n",
    "        suppression_k  = pm3.Exponential('suppression_k', 1.0     ) \n",
    "\n",
    "        max_f          = pm3.Uniform('max_f'           , 0.0, 1.0)\n",
    "        #exp_scale      = pm3.Exponential('exp_scale'   , 1.0     )        \n",
    "        \n",
    "    # now for the hyper-parameters that describe the independent fire condition covariates\n",
    "        cM  = pm3.Exponential('cM' , 1.0)\n",
    "        cMs = pm3.Exponential('cMs', 1.0)\n",
    "        cMT = pm3.Exponential('cMT', 1.0)\n",
    "        cP  = pm3.Exponential('cP' , 1.0)\n",
    "        #cD1 = pm3.Exponential('cD1', 1.0)\n",
    "        #cD2 = pm3.Exponential('cD2', 1.0)\n",
    "        kM  = pm3.LogitNormal('kM' , 0.0, 1.0)\n",
    "        pT  = pm3.Lognormal  ('pT' , 0.0, 1.0)\n",
    "        #cPS = pm3.Exponential('cPS', 1.0)\n",
    "        #cC  = pm3.Exponential('cC' , 1.0)\n",
    "        #cCS = pm3.Exponential('cCS', 1.0)\n",
    "\n",
    "        #p0 = pm3.Exponential('p0', 1.0)\n",
    "        p0 = pm3.Uniform('p0', 0.0, 1.0)\n",
    "        pp = pm3.Lognormal('pp', 0.0, 1.0)\n",
    "        \n",
    "    # describe the standard deviation in the error term\n",
    "        sigma = pm3.HalfNormal('sigma', sd=0.1)\n",
    "        \n",
    "        \n",
    "        # transform hyper-covariates \n",
    "        f_fuel        = fuel_load(fd[\"totalVeg\"].values, fd[\"soil12\"].values, fuel_pw + 1.0, fuel_pg)\n",
    "\n",
    "        emc_w         = emc_weighted(fd[\"humid\"].values, fd[\"precip\"].values, wd_pg)\n",
    "\n",
    "        f_moisture    = moisture(fd[\"soilM_bottom\"].values, fd[\"soilM_top\"].values, \n",
    "                                 emc_w, fd[\"trees\"].values, cMs, cM, cMT, kM, pT)\n",
    "\n",
    "        f_ignition    = ignition(fd[\"pas\"].values)#, \\\n",
    "                                 #cP, cPS, cC, cCS, cD1)\n",
    "        #f_ignition   = np.log(f_ignition)\n",
    "        f_suppression = supression(fd[\"crop\"].values)#, \\\n",
    "                                  # fd[\"population_density\"].values, \\\n",
    "                                  # cD2)\n",
    "\n",
    "        # burnt area is assumed to be the product of the 4 sigmoids\n",
    "\n",
    "        prediction = max_f * np.product([tt_sigmoid(f_fuel, fuel_k, fuel_x0),\n",
    "                                 tt_sigmoid(f_moisture, - moisture_k, moisture_x0),\n",
    "                                 tt_sigmoid(f_ignition, ignition_k, ignition_x0),\n",
    "                                 tt_sigmoid(f_suppression, - suppression_k, suppression_x0)])\n",
    "        \n",
    "        pz = 1.0 - (prediction**pp) * (1.0 - p0)\n",
    "        prediction = ttLogit(prediction)\n",
    "            \n",
    "        error = pm3.DensityDist(\"error\", make_zero_inflated_normal,\n",
    "                                observed = {\"mu\": prediction, \"sigma\": sigma, \"pz\": pz, \"x\": fd[\"burnt_area\"].values})\n",
    "\n",
    "        \n",
    "        # set the step-method (criteria algorithm for moving around information space)        \n",
    "        step = pm3.Metropolis()\n",
    "\n",
    "        nChains=10\n",
    "        # do the sampling\n",
    "        mcmc_traces = pm3.sample(10000, step=step, chains = nChains) #, start=start, trace=db_save\n",
    "       \n",
    "    def strip_derived_rvs(rvs):\n",
    "        '''Remove PyMC3-generated RVs from a list'''\n",
    "\n",
    "        ret_rvs = []\n",
    "        for rv in rvs:\n",
    "            if not (re.search('_log', rv.name) or re.search('_interval', rv.name)):\n",
    "                ret_rvs.append(rv)\n",
    "        return ret_rvs\n",
    "    varnames = [rv.name for rv in strip_derived_rvs(fire_error.unobserved_RVs)]\n",
    " \n",
    "    _ = pm3.traceplot(mcmc_traces[0:len(mcmc_traces[varnames[0]]):100], var_names = varnames)\n",
    "    \n",
    "    varnames = mcmc_traces.varnames\n",
    "    def cutLastX(varname, mcmc_traces, ncut = 50):\n",
    "        vals = mcmc_traces.get_values(varname)\n",
    "        def subcut(vals, r, ncut = 50):\n",
    "            cut_np = (r+1) * round(len(vals)/nChains)\n",
    "            ncut = round(len(vals) * ncut / (nChains *100))\n",
    "            return vals[(cut_np - ncut):cut_np]\n",
    "        vals = [subcut(vals, r) for r in range(nChains)]\n",
    "        return np.array(vals).flatten()\n",
    "\n",
    "    vals = [cutLastX(i, mcmc_traces) for i in varnames]\n",
    "    vals = pd.DataFrame(np.array(vals).T, columns=varnames)\n",
    "    vals.to_csv(param_outpath + '/' + outfile, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (2 chains in 4 jobs)\n",
      "CompoundStep\n",
      ">Metropolis: [sigma]\n",
      ">Metropolis: [pp]\n",
      ">Metropolis: [p0]\n",
      ">Metropolis: [pT]\n",
      ">Metropolis: [kM]\n",
      ">Metropolis: [cP]\n",
      ">Metropolis: [cMT]\n",
      ">Metropolis: [cMs]\n",
      ">Metropolis: [cM]\n",
      ">Metropolis: [max_f]\n",
      ">Metropolis: [suppression_k]\n",
      ">Metropolis: [suppression_x0]\n",
      ">Metropolis: [ignition_k]\n",
      ">Metropolis: [ignition_x0]\n",
      ">Metropolis: [wd_pg]\n",
      ">Metropolis: [moisture_k]\n",
      ">Metropolis: [moisture_x0]\n",
      ">Metropolis: [fuel_pg]\n",
      ">Metropolis: [fuel_pw]\n",
      ">Metropolis: [fuel_k]\n",
      ">Metropolis: [fuel_x0]\n",
      "Sampling 2 chains: 100%|██████████| 5000/5000 [34:08<00:00,  1.69draws/s]\n",
      "The gelman-rubin statistic is larger than 1.4 for some parameters. The sampler did not converge.\n",
      "The estimated number of effective samples is smaller than 200 for some parameters.\n",
      "Multiprocess sampling (2 chains in 4 jobs)\n",
      "CompoundStep\n",
      ">Metropolis: [sigma]\n",
      ">Metropolis: [pp]\n",
      ">Metropolis: [p0]\n",
      ">Metropolis: [pT]\n",
      ">Metropolis: [kM]\n",
      ">Metropolis: [cP]\n",
      ">Metropolis: [cMT]\n",
      ">Metropolis: [cMs]\n",
      ">Metropolis: [cM]\n",
      ">Metropolis: [max_f]\n",
      ">Metropolis: [suppression_k]\n",
      ">Metropolis: [suppression_x0]\n",
      ">Metropolis: [ignition_k]\n",
      ">Metropolis: [ignition_x0]\n",
      ">Metropolis: [wd_pg]\n",
      ">Metropolis: [moisture_k]\n",
      ">Metropolis: [moisture_x0]\n",
      ">Metropolis: [fuel_pg]\n",
      ">Metropolis: [fuel_pw]\n",
      ">Metropolis: [fuel_k]\n",
      ">Metropolis: [fuel_x0]\n",
      "Sampling 2 chains: 100%|██████████| 5000/5000 [34:21<00:00,  1.45draws/s]\n",
      "The gelman-rubin statistic is larger than 1.4 for some parameters. The sampler did not converge.\n",
      "The estimated number of effective samples is smaller than 200 for some parameters.\n",
      "Multiprocess sampling (2 chains in 4 jobs)\n",
      "CompoundStep\n",
      ">Metropolis: [sigma]\n",
      ">Metropolis: [pp]\n",
      ">Metropolis: [p0]\n",
      ">Metropolis: [pT]\n",
      ">Metropolis: [kM]\n",
      ">Metropolis: [cP]\n",
      ">Metropolis: [cMT]\n",
      ">Metropolis: [cMs]\n",
      ">Metropolis: [cM]\n",
      ">Metropolis: [max_f]\n",
      ">Metropolis: [suppression_k]\n",
      ">Metropolis: [suppression_x0]\n",
      ">Metropolis: [ignition_k]\n",
      ">Metropolis: [ignition_x0]\n",
      ">Metropolis: [wd_pg]\n",
      ">Metropolis: [moisture_k]\n",
      ">Metropolis: [moisture_x0]\n",
      ">Metropolis: [fuel_pg]\n",
      ">Metropolis: [fuel_pw]\n",
      ">Metropolis: [fuel_k]\n",
      ">Metropolis: [fuel_x0]\n",
      "Sampling 2 chains: 100%|██████████| 5000/5000 [34:51<00:00,  1.23draws/s]\n",
      "The gelman-rubin statistic is larger than 1.4 for some parameters. The sampler did not converge.\n",
      "The estimated number of effective samples is smaller than 200 for some parameters.\n"
     ]
    }
   ],
   "source": [
    "for fd, outfile in zip(fds,os.listdir(datDir)):\n",
    "    runInference(fd, outfile) \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this gives an idea of the posterior probability distribution of each parameter, but there is an important piece of information missing, namely how posterior distribution varies between parameters. Mapping out this full posterior solution is probably a bit much, but we can see how some keep key parameters vary with each other. Here, for example, x0 and k for each of our control curves.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output parameters\n",
    "The iterations at the start are just letting the optimization settle. So we will only sample to last 50% of iterations for further analysis. We also export these to csv, which others can use to do their own analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll probably add some more interesting plots in the next couple of days. Though all the fancy maps are in the next notebook, ```make_model_output.ipynb```, which is the next notebook to head to.\n",
    "\n",
    "<div>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "<font size=\"5\">\n",
    "<a style=\"font-weight: bold; size: 5\" href=\"http://localhost:8888/notebooks/notebooks/make_model_output.ipynb\">Part 3: click here</a>\n",
    "</font>\n",
    "</center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
